{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafae86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y numpy  # Force clean slate\n",
    "%pip install numpy==1.24.4  # PyTorch-compatible version\n",
    "%pip install torch==2.1.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c235a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "## Expected output:\n",
    "# PyTorch: 2.1.0+cu118\n",
    "# CUDA available: True\n",
    "# GPU: Tesla T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71d9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Place 7scenes dataset in your drive to access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724b9319",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cp \"/content/drive/MyDrive/stairs.zip\" /content/\n",
    "%cp \"/content/drive/MyDrive/stairs.mhd\" /content/\n",
    "%cp \"/content/drive/MyDrive/stairs.raw\" /content/\n",
    "# In place of stairs, you can use any other scene from 7scenes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c1017e",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b91e1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -u \"/content/stairs.zip\" -d \"/content/\"\n",
    "\n",
    "# Change the folder name to 7scenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa09e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"/content/stairs.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -u \"/content/7scenes/seq-01.zip\" -d '/content/7scenes/'\n",
    "!unzip -u \"/content/7scenes/seq-02.zip\" -d '/content/7scenes/'\n",
    "!unzip -u \"/content/7scenes/seq-03.zip\" -d '/content/7scenes/'\n",
    "!unzip -u \"/content/7scenes/seq-04.zip\" -d '/content/7scenes/'\n",
    "!unzip -u \"/content/7scenes/seq-05.zip\" -d '/content/7scenes/'\n",
    "!unzip -u \"/content/7scenes/seq-06.zip\" -d '/content/7scenes/'\n",
    "# .\n",
    "# ...\n",
    "# !unzip -u \"/content/7scenes/seq-N.zip\" -d '/content/7scenes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68df1ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install snntorch==0.9.4 e3nn==0.5.0 numpy==1.24.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e20e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8edba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch-geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ea17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch-scatter -f https://data.pyg.org/whl/torch-2.1.0+cu118.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279fd64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "import e3nn\n",
    "from e3nn import o3\n",
    "from e3nn import nn as e3nn_nn\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.data import Data, Batch\n",
    "import kornia as kn\n",
    "import glob\n",
    "from PIL import Image\n",
    "import kornia.geometry.conversions as KG\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "from torch_scatter import scatter  # Add this line\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d0b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install SimpleITK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03205338",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78538573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "\n",
    "def tsdf_to_pointcloud(mhd_path, ply_path):\n",
    "    try:\n",
    "        tsdf = sitk.ReadImage(mhd_path)\n",
    "        tsdf_vol = sitk.GetArrayFromImage(tsdf)\n",
    "        spacing = tsdf.GetSpacing()\n",
    "        offset = tsdf.GetOrigin()\n",
    "\n",
    "        indices = np.where(np.abs(tsdf_vol) < 0.05)\n",
    "        z_idx, y_idx, x_idx = indices\n",
    "        points = np.stack([\n",
    "            x_idx * spacing[0] + offset[0],  # X coordinate\n",
    "            y_idx * spacing[1] + offset[1],  # Y coordinate\n",
    "            z_idx * spacing[2] + offset[2]   # Z coordinate\n",
    "        ], axis=-1)\n",
    "\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(points)\n",
    "        o3d.io.write_point_cloud(ply_path, pcd)\n",
    "        print(f\"Saved {len(points)} points to {ply_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Change file name according to the scene:\n",
    "tsdf_to_pointcloud(\n",
    "    mhd_path=\"/content/stairs.mhd\",\n",
    "    ply_path=\"/content/stairs_points.ply\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ad5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import open3d as o3d\n",
    "def compute_scene_diameter(points):\n",
    "    \"\"\"Calculate scene diameter from point cloud\"\"\"\n",
    "    # Convert to tensor if needed\n",
    "    if not isinstance(points, torch.Tensor):\n",
    "        points = torch.tensor(points, dtype=torch.float32)\n",
    "\n",
    "    # Compute pairwise distances\n",
    "    dist_matrix = torch.cdist(points, points)  # [N,N]\n",
    "\n",
    "    # Find maximum distance\n",
    "    return torch.max(dist_matrix).item()\n",
    "\n",
    "pcd_tensor = o3d.t.io.read_point_cloud(\"stairs_points.ply\")\n",
    "scene_points = pcd_tensor.point[\"positions\"].numpy()/1000.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    batch_size = 16\n",
    "    num_steps = 32  # Increased temporal resolution\n",
    "    input_size = (128, 128)\n",
    "    num_epochs = 100\n",
    "    lr = 3e-4\n",
    "    beta = 0.97  # Slower membrane decay\n",
    "    grad_clip = 1.0\n",
    "    irreps_in = o3.Irreps(\"256x0e\")\n",
    "    irreps_hidden = o3.Irreps(\"128x0e + 64x1o + 32x2e\")\n",
    "    irreps_out = o3.Irreps(\"1o + 1o\")  # Rotation + Translation\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class LearnableTemporalPool(nn.Module):\n",
    "    def __init__(self, num_steps):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=1,  # Process each feature independently\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.weights = nn.Parameter(torch.ones(num_steps))\n",
    "\n",
    "    def forward(self, spikes):\n",
    "        batch_size, num_steps, channels, height, width = spikes.shape\n",
    "\n",
    "        # Flatten spatial and channel dimensions\n",
    "        x = spikes.reshape(batch_size * channels * height * width, num_steps)\n",
    "\n",
    "        # Add channel dimension: [B*C*H*W, T] -> [B*C*H*W, 1, T]\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Apply convolution: [B*C*H*W, 1, T] -> [B*C*H*W, 1, T]\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Apply learnable weights: [B*C*H*W, 1, T] * [1, 1, T] -> [B*C*H*W, 1, T]\n",
    "        x = x * self.weights.view(1, 1, -1)\n",
    "\n",
    "        # Sum over time: [B*C*H*W, 1]\n",
    "        x = x.sum(dim=2)\n",
    "\n",
    "        # Reshape to original spatial dimensions: [B, C, H, W]\n",
    "        return x.reshape(batch_size, channels, height, width)\n",
    "\n",
    "class SNNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64, output_dim=32, beta=0.5, num_steps=25, spike_grad=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_dim, hidden_dim, kernel_size=5, stride=2, padding=2, padding_mode='replicate')\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim*2, kernel_size=5, stride=2, padding=2, padding_mode='replicate')\n",
    "        self.conv3 = nn.Conv2d(hidden_dim*2, output_dim, kernel_size=5, stride=2, padding=2, padding_mode='replicate')\n",
    "\n",
    "        self.temporal_pool = LearnableTemporalPool(num_steps)\n",
    "\n",
    "        # Spiking neuron layers\n",
    "        spike_grad = surrogate.fast_sigmoid() if spike_grad is None else spike_grad\n",
    "        self.lif1 = snn.Leaky(\n",
    "            beta=0.85,\n",
    "            threshold=0.5,  # Lower initial threshold\n",
    "            learn_threshold=True,  # Make threshold trainable\n",
    "            spike_grad=surrogate.fast_sigmoid(slope=10.0)\n",
    "        )\n",
    "        self.lif2 = snn.Leaky(\n",
    "            beta=0.78,\n",
    "            threshold=0.6,  # Lower initial threshold\n",
    "            learn_threshold=True,  # Make threshold trainable\n",
    "            spike_grad=surrogate.fast_sigmoid(slope=10.0)\n",
    "        )\n",
    "        self.lif3 = snn.Leaky(\n",
    "            beta=0.72,\n",
    "            threshold=0.7,  # Lower initial threshold\n",
    "            learn_threshold=True,  # Make threshold trainable\n",
    "            spike_grad=surrogate.fast_sigmoid(slope=10.0)\n",
    "        )\n",
    "        # Hidden states\n",
    "        self.mem1 = self.mem2 = self.mem3 = None\n",
    "\n",
    "    def forward(self, x, num_steps=25):\n",
    "        # Reset membrane potentials\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1)  # [B, 1, C, H, W]\n",
    "        x = x.repeat(1, num_steps, 1, 1, 1)  # [B, T, C, H, W]\n",
    "        x += torch.randn_like(x) * 0.1  # Add temporal noise\n",
    "        x = x.view(batch_size * num_steps, *x.shape[2:])  # [B*T, C, H, W]\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN in SNN input after reshaping!\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        # Initialize membrane potentials\n",
    "        mem1 = torch.zeros(\n",
    "            batch_size,\n",
    "            num_steps,\n",
    "            self.conv1.out_channels,\n",
    "            x.size(2)//2,\n",
    "            x.size(3)//2\n",
    "        ).to(x.device)\n",
    "        mem2 = torch.zeros(\n",
    "            batch_size,\n",
    "            num_steps,\n",
    "            self.conv2.out_channels,\n",
    "            x.size(2)//4,\n",
    "            x.size(3)//4\n",
    "        ).to(x.device)\n",
    "        mem3 = torch.zeros(\n",
    "            batch_size,\n",
    "            num_steps,\n",
    "            self.conv3.out_channels,\n",
    "            x.size(2)//8,\n",
    "            x.size(3)//8\n",
    "        ).to(x.device)\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        spk3_rec = []\n",
    "        for t in range(num_steps):\n",
    "            x_t = x.view(batch_size, num_steps, *x.shape[1:])[:, t]\n",
    "\n",
    "            # Layer 1 with snnTorch state handling\n",
    "            cur1 = self.conv1(x_t)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)  # Automatic state update\n",
    "\n",
    "            # Layer 2\n",
    "            cur2 = self.conv2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            # Layer 3\n",
    "            cur3 = self.conv3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=1)\n",
    "        \n",
    "class SpikeToGeometric(nn.Module):\n",
    "    def __init__(self, input_dim=32, output_scalar=32, output_vector=32):\n",
    "        super().__init__()\n",
    "        self.conv_scalar = nn.Conv2d(input_dim, output_scalar, kernel_size=1)\n",
    "        self.conv_vector = nn.Conv2d(input_dim, output_vector*3, kernel_size=1)\n",
    "        self.output_vector = output_vector\n",
    "\n",
    "        # Proper initialization for Conv layers\n",
    "        nn.init.kaiming_normal_(self.conv_scalar.weight,\n",
    "                              mode='fan_in',\n",
    "                              nonlinearity='leaky_relu')\n",
    "        nn.init.kaiming_normal_(self.conv_vector.weight,\n",
    "                              mode='fan_in',\n",
    "                              nonlinearity='leaky_relu')\n",
    "        nn.init.zeros_(self.conv_scalar.bias)\n",
    "        nn.init.zeros_(self.conv_vector.bias)\n",
    "\n",
    "    def forward(self, spike_features):\n",
    "        \"\"\"Convert spike rate features to geometric features with spatial preservation\"\"\"\n",
    "        # Input shape: [B, 32, 16, 16]\n",
    "        # Process scalar features (per-location invariant features)\n",
    "        batch_size = spike_features.size(0)\n",
    "        if torch.isnan(spike_features).any():\n",
    "            print(\"NaN in spike_features input to SpikeToGeometric\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        scalar_features = self.conv_scalar(spike_features)  # [B,64,16,16]\n",
    "        if torch.isnan(scalar_features).any():\n",
    "            print(\"NaN in scalar_features after conv_scalar\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        # Process vector features (per-location equivariant features)\n",
    "        vector_features = self.conv_vector(spike_features)  # [B,192,16,16]\n",
    "        vector_features = vector_features.view(\n",
    "            batch_size, self.output_vector, 3, 16, 16\n",
    "        )  # [B,64,3,16,16]\n",
    "        if torch.isnan(vector_features).any():\n",
    "            print(\"NaN in vector_features after conv_vector\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        return scalar_features, vector_features\n",
    "\n",
    "class GraphConstructor(nn.Module):\n",
    "    def __init__(self, feature_dim=256, k_neighbors=8):\n",
    "        super().__init__()\n",
    "        self.k = k_neighbors\n",
    "        self.feature_projection = nn.Linear(feature_dim, feature_dim)#.to(Config.device)\n",
    "\n",
    "    def forward(self, features, positions=None):\n",
    "        batch_size, num_nodes, _ = features.size()\n",
    "        device = features.device\n",
    "\n",
    "        # Project features\n",
    "        node_features = self.feature_projection(features)\n",
    "\n",
    "        # Build graphs\n",
    "        graphs = []\n",
    "        for b in range(batch_size):\n",
    "            if positions is not None:\n",
    "                # Use 3D spatial positions for KNN\n",
    "                pos = positions[b]\n",
    "                edge_index = self._knn_graph(pos, self.k)\n",
    "            else:\n",
    "                # Fallback to feature similarity graph\n",
    "                feat = node_features[b]\n",
    "                edge_index = self._feature_graph(feat, self.k)\n",
    "\n",
    "            graph = Data(\n",
    "                x=node_features[b],\n",
    "                edge_index=edge_index,\n",
    "                pos=positions[b] if positions is not None else None\n",
    "            )\n",
    "            graphs.append(graph)\n",
    "\n",
    "        return Batch.from_data_list(graphs)\n",
    "\n",
    "    def _feature_graph(self, features, k):\n",
    "        \"\"\"Construct graph based on feature similarity (critical for non-spatial data)\"\"\"\n",
    "        device = features.device\n",
    "        num_nodes = features.size(0)\n",
    "        effective_k = min(k, num_nodes - 1)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        features_norm = F.normalize(features, p=2, dim=1)\n",
    "        similarity = torch.mm(features_norm, features_norm.t())\n",
    "\n",
    "        # Get top k+1 similar neighbors\n",
    "        _, indices = torch.topk(similarity, k=effective_k+1, largest=True)\n",
    "        indices = indices[:, 1:]  # Remove self-loops\n",
    "\n",
    "        # Create edge index with proper device handling\n",
    "        rows = torch.arange(num_nodes, device=device)[:, None].expand(-1, effective_k)\n",
    "        edge_index = torch.stack([\n",
    "            rows.reshape(-1),\n",
    "            indices.reshape(-1).to(device)\n",
    "        ], dim=0)\n",
    "\n",
    "        return edge_index\n",
    "\n",
    "    def _knn_graph(self, points, k):\n",
    "        \"\"\"3D spatial KNN graph construction with locality preservation\"\"\"\n",
    "        device = points.device\n",
    "        num_nodes = points.size(0)\n",
    "        effective_k = min(k, num_nodes - 1)\n",
    "\n",
    "        # Preserve spatial locality by using grid ordering\n",
    "        if num_nodes == 256:  # 16x16 grid\n",
    "            # Create grid indices\n",
    "            idx = torch.arange(num_nodes, device=device).view(16, 16)\n",
    "\n",
    "            # Horizontal connections\n",
    "            horizontal_edges = []\n",
    "            for i in range(16):\n",
    "                for j in range(15):\n",
    "                    n1 = idx[i, j]\n",
    "                    n2 = idx[i, j+1]\n",
    "                    horizontal_edges.append([n1, n2])\n",
    "                    horizontal_edges.append([n2, n1])\n",
    "\n",
    "            # Vertical connections\n",
    "            vertical_edges = []\n",
    "            for i in range(15):\n",
    "                for j in range(16):\n",
    "                    n1 = idx[i, j]\n",
    "                    n2 = idx[i+1, j]\n",
    "                    vertical_edges.append([n1, n2])\n",
    "                    vertical_edges.append([n2, n1])\n",
    "\n",
    "            # Diagonal connections\n",
    "            diagonal_edges = []\n",
    "            for i in range(15):\n",
    "                for j in range(15):\n",
    "                    n1 = idx[i, j]\n",
    "                    n2 = idx[i+1, j+1]\n",
    "                    diagonal_edges.append([n1, n2])\n",
    "                    diagonal_edges.append([n2, n1])\n",
    "\n",
    "            # Combine all connections\n",
    "            edge_index = torch.tensor(horizontal_edges + vertical_edges + diagonal_edges,\n",
    "                                     device=device).t().contiguous()\n",
    "            return edge_index\n",
    "\n",
    "        # Fallback to geometric KNN for non-grid structures\n",
    "        dist = torch.cdist(points, points)\n",
    "        _, indices = torch.topk(dist, k=effective_k+1, largest=False)\n",
    "        indices = indices[:, 1:]  # Remove self-loops\n",
    "\n",
    "        rows = torch.arange(num_nodes, device=device)[:, None].expand(-1, effective_k)\n",
    "        return torch.stack([rows.reshape(-1), indices.reshape(-1)], dim=0)\n",
    "\n",
    "class SE3EquivariantGNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=32, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.irreps_in = o3.Irreps(\"64x0e + 64x1o\")\n",
    "        self.irreps_hidden = o3.Irreps(\"64x0e + 64x1o\")\n",
    "        self.irreps_edge = o3.Irreps(\"1x0e\")  # Scalar edge features\n",
    "        self.irreps_out = o3.Irreps(\"1x1o + 2x1o\")\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Initial layer with edge integration\n",
    "        self.layers.append(o3.TensorProduct(\n",
    "            self.irreps_in,\n",
    "            self.irreps_edge,\n",
    "            self.irreps_hidden,\n",
    "            instructions=[(i,0,i,'uvu',True, 1.0) for i in range(len(self.irreps_in))],\n",
    "            shared_weights=True,\n",
    "            internal_weights=True\n",
    "        ))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers-2):\n",
    "            self.layers.append(o3.TensorProduct(\n",
    "                self.irreps_hidden,\n",
    "                self.irreps_edge,\n",
    "                self.irreps_hidden,\n",
    "                instructions=[(i,0,i,'uvu', True, 1.0) for i in range(len(self.irreps_hidden))],\n",
    "                shared_weights=True,\n",
    "                path_normalization='element',\n",
    "                internal_weights=True\n",
    "            ))\n",
    "            self.layers.append(self.Float32NormActivation(\n",
    "                self.irreps_hidden,\n",
    "                scalar_nonlinearity=torch.tanh,\n",
    "                epsilon=1e-5\n",
    "            ))\n",
    "            \n",
    "        # Output layer\n",
    "        self.layers.append(o3.Linear(self.irreps_hidden, self.irreps_out))\n",
    "\n",
    "    class Float32NormActivation(e3nn_nn.NormActivation):\n",
    "        def forward(self, features):\n",
    "            original_dtype = features.dtype\n",
    "            # Convert to float32 for stable computation\n",
    "            features = features.float()\n",
    "            features = super().forward(features)\n",
    "            # Convert back to original precision\n",
    "            return features.to(original_dtype)\n",
    "\n",
    "    def forward(self, x, edge_index, pos):\n",
    "        # Compute edge features (radial basis)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN in GNN input\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        senders, receivers = edge_index\n",
    "        rel_pos = pos[senders] - pos[receivers]\n",
    "        distances = torch.norm(rel_pos, dim=1, keepdim=True)\n",
    "        if torch.isnan(rel_pos).any() or torch.isnan(distances).any():\n",
    "            print(\"NaN in edge features\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        edge_attr = torch.exp(-distances**2)  # [num_edges, 1]\n",
    "        # Message passing loop\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, o3.TensorProduct):\n",
    "                # Aggregate messages using edge features\n",
    "                messages = layer(x[senders], edge_attr)\n",
    "                if torch.isnan(messages).any():\n",
    "                    print(f\"NaN in messages at layer {layer_idx}\")\n",
    "                    raise RuntimeError\n",
    "\n",
    "                x = scatter(messages, receivers, dim=0, dim_size=x.size(0))\n",
    "                if torch.isnan(x).any():\n",
    "                    print(f\"NaN after scattering at layer {layer_idx}\")\n",
    "                    raise RuntimeError\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                # print(x)\n",
    "                if torch.isnan(x).any():\n",
    "                    print(x)\n",
    "                    print(f\"NaN after layer {layer_idx} ({type(layer).__name__})\")\n",
    "                    raise RuntimeError\n",
    "\n",
    "        # Split output\n",
    "        translation = x[:, :3]  # First 3 components (translation)\n",
    "        rotation_6d = x[:, 3:9]  # Raw 6D representation\n",
    "        return translation, rotation_6d\n",
    "\n",
    "    def inference_orthogonalize(self, rotation_6d):\n",
    "        \"\"\"Apply orthogonalization ONLY during inference\"\"\"\n",
    "        a1, a2 = rotation_6d[:, :3], rotation_6d[:, 3:6]\n",
    "        return self.sixd_to_rotation_matrix(a1, a2)\n",
    "\n",
    "    def sixd_to_rotation_matrix(self, a1, a2):\n",
    "        \"\"\"Stable 6D → rotation conversion\"\"\"\n",
    "        # Add epsilon guards\n",
    "        a1 = a1 / (torch.norm(a1, dim=-1, keepdim=True) + 1e-6)\n",
    "        a2 = a2 - (a1 * a2).sum(dim=-1, keepdim=True) * a1\n",
    "        a2 = a2 / (torch.norm(a2, dim=-1, keepdim=True) + 1e-6)\n",
    "        b3 = torch.cross(a1, a2)\n",
    "        R = torch.stack([a1, a2, b3], dim=-1)\n",
    "\n",
    "        return R\n",
    "\n",
    "class NeuromorphicLieGNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, hidden_dim=64, num_steps=25, beta=0.5, num_gnn_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # SNN encoder\n",
    "        self.snn_encoder = SNNEncoder(\n",
    "            input_dim=input_channels,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=32,\n",
    "            beta=beta\n",
    "        )\n",
    "\n",
    "        # Spike to geometric conversion\n",
    "        self.spike_to_geo = SpikeToGeometric(\n",
    "            input_dim=32,  # Depends on input size and encoder architecture\n",
    "            output_scalar=64,\n",
    "            output_vector=64\n",
    "        )\n",
    "\n",
    "        # Graph construction\n",
    "        self.graph_constructor = GraphConstructor(feature_dim=256, k_neighbors=8)\n",
    "\n",
    "        # SE(3) equivariant GNN\n",
    "        self.se3_gnn = SE3EquivariantGNN(hidden_dim=32, num_layers=num_gnn_layers)\n",
    "\n",
    "        # Parameters\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def forward(self, images, positions=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Batch of input images (B, C, H, W)\n",
    "            positions: Optional initial position estimates (B, N, 3)\n",
    "        \"\"\"\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # 1. Process through SNN encoder to get spike trains\n",
    "        spike_features = self.snn_encoder(images, self.num_steps)\n",
    "        # print(\"Encoder spikes:\", torch.mean(spike_features).item())\n",
    "        # print(\"Spike features shape:\", spike_features.shape)  # [B,T,C,H,W]  # Add here\n",
    "        if torch.isnan(spike_features).any():\n",
    "            raise ValueError(\"NaN in spike features!\")\n",
    "\n",
    "        # 2. Convert to rate-based representation\n",
    "        rate_features = self.snn_encoder.temporal_pool(spike_features)\n",
    "        # Ensure consistent dtype\n",
    "        rate_features = rate_features.to(torch.float32)\n",
    "        # print(\"Rate features shape:\", rate_features.shape)  # [B,C,H,W]\n",
    "        if torch.isnan(rate_features).any():\n",
    "            raise ValueError(\"NaN in rate features!\")\n",
    "\n",
    "        # 3. Convert to geometric features\n",
    "        scalar_features, vector_features = self.spike_to_geo(rate_features)\n",
    "        if torch.isnan(scalar_features).any() or torch.isnan(vector_features).any():\n",
    "            print(\"NaN in geometric features\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        # print(\"Scalar features:\", scalar_features.abs().mean().item())\n",
    "        # print(\"Vector features:\", vector_features.abs().mean().item())\n",
    "        # print(\"Vector features pre-reshape:\", vector_features.shape)\n",
    "        vector_reshaped = vector_features.view(\n",
    "            batch_size,\n",
    "            self.spike_to_geo.output_vector * 3,  # 64*3=192\n",
    "            16,\n",
    "            16\n",
    "        )\n",
    "        # print(\"Vector features post-reshape:\", vector_reshaped.shape)  # [B,192,16,16]\n",
    "        # 4. Construct graph\n",
    "        # Combine scalar and vector features for graph construction\n",
    "        combined_features = torch.cat([\n",
    "            scalar_features,  # [B,64,16,16]\n",
    "            vector_reshaped\n",
    "        ], dim=1)  # [B,256,16,16]\n",
    "        # print(\"Combined features shape:\", combined_features.shape)  # [B,256,16,16]\n",
    "\n",
    "        # Reshape to spatial nodes [B,256,16,16] -> [B,256,256]\n",
    "        node_features = combined_features.permute(0, 2, 3, 1)  # [B, 16, 16, 256]\n",
    "\n",
    "        # Get normalized grid positions\n",
    "        grid = self._get_grid_positions().to(images.device)\n",
    "        grid = (grid - grid.min()) / (grid.max() - grid.min()) * 2 - 1  # Normalize to [-1,1]\n",
    "\n",
    "        # Expand positions to match batch size\n",
    "        positions = grid.unsqueeze(0).repeat(batch_size, 1, 1, 1)  # [B, 16, 16, 3]\n",
    "\n",
    "        # Flatten spatial dimensions while preserving locality\n",
    "        node_features = node_features.reshape(batch_size, 256, -1)  # [B, 256, 256]\n",
    "        positions = positions.reshape(batch_size, 256, 3)  # [B, 256, 3]\n",
    "\n",
    "        # Build graph with spatial positions\n",
    "        graph = self.graph_constructor(node_features, positions)\n",
    "\n",
    "        # 5. Process through SE(3) equivariant GNN\n",
    "        position, rotation_6d = self.se3_gnn(graph.x, graph.edge_index, graph.pos)\n",
    "\n",
    "\n",
    "        rotation_matrix = self.se3_gnn.inference_orthogonalize(rotation_6d)\n",
    "\n",
    "        position = scatter(position, graph.batch, dim=0, reduce='mean')\n",
    "        rotation_6d = scatter(rotation_6d, graph.batch, dim=0, reduce='mean')\n",
    "        rotation_matrix = scatter(rotation_matrix, graph.batch, dim=0, reduce='mean')\n",
    "\n",
    "        return {\n",
    "            'position': position,\n",
    "            'rotation_6d': rotation_6d,  # For training loss\n",
    "            'rotation_matrix': rotation_matrix,  # For inference/validation\n",
    "            'pose': self._combine_pose(position, rotation_matrix)\n",
    "        }\n",
    "\n",
    "    def _combine_pose(self, position, rotation):\n",
    "        \"\"\"Combine position and rotation into 4x4 transformation matrix\"\"\"\n",
    "        batch_size = position.size(0)\n",
    "\n",
    "        # Convert quaternion to rotation matrix\n",
    "        R = rotation\n",
    "        # Create transformation matrix\n",
    "        T = torch.eye(4).unsqueeze(0).repeat(batch_size, 1, 1).to(position.device)\n",
    "        T[:, :3, :3] = R\n",
    "        T[:, :3, 3] = position\n",
    "\n",
    "        return T\n",
    "\n",
    "    def _get_grid_positions(self):\n",
    "        \"\"\"Generate 3D pseudo-coordinates for spatial graph construction\"\"\"\n",
    "        y, x = torch.meshgrid(\n",
    "            torch.arange(16),\n",
    "            torch.arange(16),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        z = torch.zeros_like(x)  # Add depth dimension\n",
    "        return torch.stack([x.float(), y.float(), z], dim=-1)  # [16,16,3]\n",
    "\n",
    "\n",
    "class PoseLoss(nn.Module):\n",
    "    def __init__(self, scene_points, alpha=0.9):\n",
    "        super().__init__()\n",
    "        # Scene points setup\n",
    "        self.register_buffer('scene_points', self._preprocess_scene_points(scene_points))\n",
    "\n",
    "        # EMA parameters for adaptive weighting\n",
    "        self.alpha = alpha\n",
    "        self.register_buffer('pos_ema', torch.tensor(float('inf')))\n",
    "        self.register_buffer('rot_ema', torch.tensor(float('inf')))\n",
    "        self.register_buffer('add_ema', torch.tensor(float('inf')))\n",
    "\n",
    "    def _farthest_point_sampling(self, points, n_samples):\n",
    "        \"\"\"Memory-efficient FPS without full distance matrix\"\"\"\n",
    "        device = points.device\n",
    "        n_points = points.shape[0]\n",
    "\n",
    "        # Initialize with random point\n",
    "        start_idx = torch.randint(0, n_points, (1,)).item()\n",
    "        selected_indices = [start_idx]\n",
    "\n",
    "        # Initialize min distances\n",
    "        min_dists = torch.full((n_points,), float('inf'), device=device)\n",
    "\n",
    "        # Compute initial distances\n",
    "        dists = torch.norm(points - points[start_idx], dim=1)\n",
    "        min_dists = torch.min(min_dists, dists)\n",
    "\n",
    "        for _ in range(1, min(n_samples, n_points)):\n",
    "            # Find farthest point\n",
    "            next_idx = torch.argmax(min_dists).item()\n",
    "            selected_indices.append(next_idx)\n",
    "\n",
    "            # Compute distances to new point\n",
    "            new_dists = torch.norm(points - points[next_idx], dim=1)\n",
    "\n",
    "            # Update min distances\n",
    "            min_dists = torch.min(min_dists, new_dists)\n",
    "            min_dists[next_idx] = 0  # Avoid reselecting\n",
    "\n",
    "        return points[selected_indices]\n",
    "\n",
    "    def _preprocess_scene_points(self, points):\n",
    "        \"\"\"Fast random subsampling for debugging\"\"\"\n",
    "        if isinstance(points, np.ndarray):\n",
    "            points = torch.from_numpy(points)\n",
    "        points = points.float()\n",
    "\n",
    "        # Simple random sampling instead of FPS\n",
    "        if len(points) > 500:\n",
    "            indices = torch.randperm(len(points))[:500]\n",
    "            points = points[indices]\n",
    "\n",
    "        return points\n",
    "\n",
    "    def _matrix_to_6d(self, rotation_matrix):\n",
    "        \"\"\"Convert rotation matrix to 6D representation\"\"\"\n",
    "        return rotation_matrix[:, :, :2].reshape(rotation_matrix.size(0), 6)\n",
    "\n",
    "    def compute_add_s(self, pred_trans, pred_rot, gt_trans, gt_rot):\n",
    "        scene_points = self.scene_points.to(pred_trans.device)\n",
    "        batch_size = pred_trans.size(0)\n",
    "\n",
    "        # Expand points to [B, N, 3]\n",
    "        scene_points_batch = scene_points.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "        # Stable transformation (avoid einsum)\n",
    "        pred_points = torch.bmm(pred_rot, scene_points_batch.transpose(1,2)).transpose(1,2) + pred_trans.unsqueeze(1)\n",
    "        gt_points = torch.bmm(gt_rot, scene_points_batch.transpose(1,2)).transpose(1,2) + gt_trans.unsqueeze(1)\n",
    "\n",
    "        # Clamped distances\n",
    "        dists = torch.cdist(pred_points, gt_points)\n",
    "        min_dists = dists.min(dim=-1)[0].clamp(max=1.0)  # Prevent outliers\n",
    "        return min_dists.mean(dim=1).mean()\n",
    "\n",
    "    def forward(self, pred, target, epoch):\n",
    "        # Position loss\n",
    "        pos_loss = F.smooth_l1_loss(pred['position'], target['position'])\n",
    "\n",
    "        # Rotation loss - direct 6D comparison\n",
    "        R_pred = self.sixd_to_rotation_matrix(pred['rotation_6d'])\n",
    "\n",
    "        # Geodesic rotation loss\n",
    "        rot_loss = self.geodesic_loss(R_pred, target['rotation_matrix'])\n",
    "\n",
    "        # ADD-S loss with orthogonalized rotation\n",
    "        add_s = self.compute_add_s(\n",
    "            pred['position'], R_pred,\n",
    "            target['position'], target['rotation_matrix']\n",
    "        )\n",
    "\n",
    "        # Update EMAs\n",
    "        self.pos_ema = self.alpha * self.pos_ema + (1 - self.alpha) * pos_loss.detach()\n",
    "        self.rot_ema = self.alpha * self.rot_ema + (1 - self.alpha) * rot_loss.detach()\n",
    "        self.add_ema = self.alpha * self.add_ema + (1 - self.alpha) * add_s.detach()\n",
    "\n",
    "        # Compute stable weights with clipping\n",
    "        w_pos = torch.clamp(1.0 / (self.pos_ema + 1e-3), 0.1, 10.0)\n",
    "        w_rot = torch.clamp(1.0 / (self.rot_ema + 1e-3), 0.1, 10.0)\n",
    "        w_add = torch.clamp(1.0 / (self.add_ema + 1e-3), 0.1, 10.0)\n",
    "\n",
    "        # Normalize\n",
    "        total = w_pos + w_rot + w_add + 1e-7\n",
    "        w_pos /= total\n",
    "        w_rot /= total\n",
    "        w_add /= total\n",
    "\n",
    "        total_loss = w_pos * pos_loss + w_rot * rot_loss + w_add * add_s\n",
    "\n",
    "        return {\n",
    "            'total': total_loss,\n",
    "            'position': pos_loss,\n",
    "            'rotation': rot_loss,\n",
    "            'add_s': add_s,\n",
    "            'weights': {\n",
    "                'pos': w_pos.item(),\n",
    "                'rot': w_rot.item(),\n",
    "                'add': w_add.item()\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def geodesic_loss(self, R_pred, R_target):\n",
    "        \"\"\"Stable geodesic distance on SO(3)\"\"\"\n",
    "        R_diff = torch.bmm(R_pred, R_target.transpose(1, 2))\n",
    "        trace = R_diff[:, 0, 0] + R_diff[:, 1, 1] + R_diff[:, 2, 2]\n",
    "        angle = torch.acos(torch.clamp((trace - 1)/2, -1+1e-6, 1-1e-6))\n",
    "        return angle.mean()\n",
    "\n",
    "    def sixd_to_rotation_matrix(self, rotation_6d):\n",
    "        \"\"\"SVD-based stable orthogonalization\"\"\"\n",
    "        a1, a2 = rotation_6d[:, :3], rotation_6d[:, 3:6]\n",
    "\n",
    "        # Form unnormalized matrix\n",
    "        b3 = torch.cross(a1, a2)\n",
    "        R = torch.stack([a1, a2, b3], dim=-1)\n",
    "\n",
    "        # SVD orthogonalization\n",
    "        U, S, Vh = torch.linalg.svd(R)\n",
    "        return U @ Vh.transpose(-1, -2)\n",
    "\n",
    "class SevenScenesPoseDataset(Dataset):\n",
    "    def __init__(self, roots=['/content/7scenes', '/content/7scenes2', '/content/7scenes3'],\n",
    "                 transform=None, split='train', val_test_split_ratio=0.7):\n",
    "        self.roots = roots\n",
    "        self.split = split\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(Config.input_size),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # NEW\n",
    "            transforms.RandomRotation(5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],  # RGB means\n",
    "                std=[0.229, 0.224, 0.225]     # RGB stds\n",
    "            )\n",
    "        ])\n",
    "        self.val_test_split_ratio = val_test_split_ratio\n",
    "\n",
    "        # Initialize data storage\n",
    "        self.poses_t = []\n",
    "        self.poses_r = []\n",
    "        self.samples = []\n",
    "\n",
    "        def format_sequence_name(seq_str):\n",
    "            \"\"\"Convert raw sequence strings to standardized seq-XX format\"\"\"\n",
    "            # Remove existing 'seq-' prefix if present\n",
    "            clean_str = seq_str.lower().replace('sequence', '').strip()\n",
    "\n",
    "            # Handle numeric formats\n",
    "            if clean_str.isdigit():\n",
    "                seq_num = int(clean_str)\n",
    "                return f\"seq-{seq_num:02d}\"  # Always 2-digit format\n",
    "\n",
    "            # Handle already formatted seq-01 cases\n",
    "            if len(clean_str) == 2 and clean_str.isdigit():\n",
    "                return f\"seq-{clean_str}\"\n",
    "\n",
    "            # Fallback for non-numeric sequences\n",
    "            return f\"seq-{clean_str.zfill(2)}\"\n",
    "\n",
    "        # Collect sequences from all roots\n",
    "        all_train_sequences = []\n",
    "        all_test_sequences = []\n",
    "\n",
    "        for root in self.roots:\n",
    "            train_split_file = os.path.join(root, \"TrainSplit.txt\")\n",
    "            test_split_file = os.path.join(root, \"TestSplit.txt\")\n",
    "\n",
    "            # Load train sequences\n",
    "            if os.path.exists(train_split_file):\n",
    "                with open(train_split_file) as f:\n",
    "                    for line in f:\n",
    "                            raw_seq = line.strip()\n",
    "                            if raw_seq:\n",
    "                                # Format sequence name before adding\n",
    "                                formatted_seq = format_sequence_name(raw_seq)\n",
    "                                all_train_sequences.append((root, formatted_seq))\n",
    "\n",
    "            # Load test sequences\n",
    "            if os.path.exists(test_split_file):\n",
    "                with open(test_split_file) as f:\n",
    "                    for line in f:\n",
    "                            raw_seq = line.strip()\n",
    "                            if raw_seq:\n",
    "                                # Format sequence name before adding\n",
    "                                formatted_seq = format_sequence_name(raw_seq)\n",
    "                                all_test_sequences.append((root, formatted_seq))\n",
    "\n",
    "        # Split validation and test from test sequences\n",
    "        if split in ['val', 'test']:\n",
    "            # Split test sequences into val/test subsets\n",
    "            split_idx = int(len(all_test_sequences) * self.val_test_split_ratio)\n",
    "            if split == 'val':\n",
    "                selected_sequences = all_test_sequences[:split_idx]\n",
    "            else:\n",
    "                selected_sequences = all_test_sequences[split_idx:]\n",
    "        else:\n",
    "            selected_sequences = all_train_sequences\n",
    "\n",
    "        for root, seq in selected_sequences:\n",
    "            seq_num = seq[-1]\n",
    "            seq_name = 'seq-0'+seq_num\n",
    "            seq = seq_name\n",
    "        print(selected_sequences)\n",
    "        # Process selected sequences\n",
    "        for root, seq in selected_sequences:\n",
    "            seq_path = os.path.join(root, seq)\n",
    "            if not os.path.exists(seq_path):\n",
    "                continue\n",
    "\n",
    "            # Extract numeric sequence ID\n",
    "            seq_id = seq.split('-')[-1]\n",
    "\n",
    "            # Collect valid samples\n",
    "            color_files = sorted([\n",
    "                f for f in os.listdir(seq_path)\n",
    "                if f.endswith('color.png')\n",
    "            ])\n",
    "\n",
    "            for cf in color_files:\n",
    "                base = cf.replace('color.png', '')\n",
    "                pose_file = base + 'pose.txt'\n",
    "                pose_path = os.path.join(seq_path, pose_file)\n",
    "                img_path = os.path.join(seq_path, cf)\n",
    "\n",
    "                if os.path.exists(pose_path):\n",
    "                    # Load and process pose\n",
    "                    pose_mat = np.loadtxt(pose_path)\n",
    "                    translation = pose_mat[:3, 3]\n",
    "                    rotation = pose_mat[:3, :3]\n",
    "\n",
    "                    self.poses_t.append(torch.tensor(translation))\n",
    "                    self.poses_r.append(torch.tensor(rotation))\n",
    "                    self.samples.append(img_path)\n",
    "\n",
    "        # Convert to tensors\n",
    "        if self.poses_t:\n",
    "            self.poses_t = torch.stack(self.poses_t)\n",
    "            self.poses_r = torch.stack(self.poses_r)\n",
    "        else:\n",
    "            self.poses_t = torch.empty(0)\n",
    "            self.poses_r = torch.empty(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.samples[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # Process rotation matrix\n",
    "        rotation = self.poses_r[idx].float()\n",
    "        translation = self.poses_t[idx].float()\n",
    "\n",
    "        return {\n",
    "            'image': self.transform(image).float(),\n",
    "            'rotation_matrix': rotation,\n",
    "            'translation': translation.clone().detach(),\n",
    "            'path': img_path \n",
    "        }\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, epoch, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        model.snn_encoder.lif1.reset_mem()\n",
    "        model.snn_encoder.lif2.reset_mem()\n",
    "        model.snn_encoder.lif3.reset_mem()\n",
    "\n",
    "        # Move data to device\n",
    "        images = batch['image'].to(device, dtype=torch.float32)\n",
    "        target_position = batch['translation'].to(device, dtype=torch.float32)\n",
    "        target_rotation = batch['rotation_matrix'].to(device, dtype=torch.float32)\n",
    "\n",
    "        # Skip any NaN batches\n",
    "        if torch.isnan(images).any() or torch.isnan(target_position).any() or torch.isnan(target_rotation).any():\n",
    "            continue\n",
    "\n",
    "        # 1. Forward + loss under autocast\n",
    "        #with torch.cuda.amp.autocast():  # ← AMP enabled here\n",
    "        outputs = model(images)\n",
    "        pred = {\n",
    "            'position': outputs['position'],\n",
    "            'rotation_6d': outputs['rotation_6d']\n",
    "        }\n",
    "        loss_dict = criterion(pred, {'position': target_position, 'rotation_matrix': target_rotation}, epoch)\n",
    "        loss = loss_dict['total'] / accumulation_steps\n",
    "\n",
    "        # 2. Backward scaled loss\n",
    "        retain_graph = (batch_idx + 1) % accumulation_steps != 0\n",
    "        scaler.scale(loss).backward(retain_graph=retain_graph)\n",
    "\n",
    "        # 3. Step, unscale, clip, update only at accumulation boundary\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            # Unscale gradients before clipping\n",
    "            scaler.unscale_(optimizer)  # moves gradients back to FP32 for clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip\n",
    "\n",
    "            # Step optimizer and update scaler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss += loss_dict['total'].item()\n",
    "\n",
    "    # Handle leftover gradients if dataset size not divisible by accumulation_steps\n",
    "    if len(dataloader) % accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def compute_add_s_accuracy(pred_pts, gt_pts, thresholds=(0.1, 0.2, 0.5)):\n",
    "    \"\"\"Compute ADD-S accuracy at multiple thresholds\"\"\"\n",
    "    # Compute pairwise distances\n",
    "    dists = torch.cdist(pred_pts, gt_pts)\n",
    "\n",
    "    # Get minimum distance for each point\n",
    "    min_dists = dists.min(dim=-1)[0]\n",
    "\n",
    "    # Calculate accuracy for each threshold\n",
    "    accuracies = {}\n",
    "    for thr in thresholds:\n",
    "        within_threshold = (min_dists < thr).float()\n",
    "        accuracies[f'acc@{thr:.1f}'] = within_threshold.mean()\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "def validate(model, dataloader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    loss_sums = {'total': 0.0, 'position': 0.0, 'rotation': 0.0, 'add_s': 0.0}\n",
    "    acc_sums = {'acc@0.1': 0.0, 'acc@0.2': 0.0, 'acc@0.5': 0.0}\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['image'].to(device)\n",
    "            target_pos = batch['translation'].to(device)\n",
    "            target_rot = batch['rotation_matrix'].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, {'position': target_pos, 'rotation_matrix': target_rot}, epoch)\n",
    "\n",
    "            # Accumulate losses\n",
    "            for k in loss_sums:\n",
    "                loss_sums[k] += loss[k].item()\n",
    "\n",
    "            # Compute ADD-S accuracy\n",
    "            scene_pts = criterion.scene_points.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            pts = scene_pts.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "            # Use orthogonalized rotation for accuracy computation\n",
    "            R_pred = criterion.sixd_to_rotation_matrix(outputs['rotation_6d'])\n",
    "            pred_pts = torch.bmm(R_pred, pts.transpose(1,2)).transpose(1,2) + outputs['position'].unsqueeze(1)\n",
    "            gt_pts = torch.bmm(target_rot, pts.transpose(1,2)).transpose(1,2) + target_pos.unsqueeze(1)\n",
    "\n",
    "            batch_acc = compute_add_s_accuracy(pred_pts, gt_pts)\n",
    "\n",
    "            # Accumulate accuracies\n",
    "            for k in acc_sums:\n",
    "                acc_sums[k] += batch_acc[k].item()\n",
    "\n",
    "    # Average across batches\n",
    "    for k in loss_sums:\n",
    "        loss_sums[k] /= num_batches\n",
    "    for k in acc_sums:\n",
    "        acc_sums[k] /= num_batches\n",
    "\n",
    "    # Combine results\n",
    "    return {**loss_sums, **acc_sums}\n",
    "\n",
    "def main():\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = NeuromorphicLieGNN().to(device)\n",
    "    def create_param_groups(model):\n",
    "        rot_params = [p for n,p in model.named_parameters() if 'rot' in n]\n",
    "        trans_params = [p for n,p in model.named_parameters() if 'position' in n or 'trans' in n]\n",
    "        other_params = [p for n,p in model.named_parameters() if not ('rot' in n or 'position' in n)]\n",
    "\n",
    "        return [\n",
    "            {'params': rot_params, 'lr': 1e-4},\n",
    "            {'params': trans_params, 'lr': 3e-4},\n",
    "            {'params': other_params, 'lr': 2e-4}\n",
    "        ]\n",
    "\n",
    "    param_groups = create_param_groups(model)\n",
    "    optimizer = optim.AdamW(param_groups, weight_decay=5e-6)\n",
    "\n",
    "    # optimizer = optim.AdamW(model.parameters(), lr=Config.lr, weight_decay=5e-6)\n",
    "    scene_points = pcd_tensor.point[\"positions\"].numpy()/1000.0\n",
    "    criterion = PoseLoss(scene_points=scene_points).to(device)\n",
    "\n",
    "    train_dataset = SevenScenesPoseDataset(split='train')\n",
    "    val_dataset = SevenScenesPoseDataset(split='val')\n",
    "    test_dataset = SevenScenesPoseDataset(split='test')\n",
    "    dummy = torch.randn(2,3,128,128).to(Config.device)\n",
    "    out = model(dummy)\n",
    "    # print(f\"Position: {out['position'].min():.2f} to {out['position'].max():.2f}\")\n",
    "    # print(f\"Rotation: {out['rotation'].min():.2f} to {out['rotation'].max():.2f}\")\n",
    "\n",
    "    sample = train_dataset[0]\n",
    "\n",
    "    print(\"Image stats after normalization:\")\n",
    "    print(\"Min:\", sample['image'].min().item())\n",
    "    print(\"Max:\", sample['image'].max().item())\n",
    "    print(\"Mean:\", sample['image'].mean().item())\n",
    "\n",
    "    # print(\"\\nDataset Sanity Check:\")\n",
    "    # print(f\"Quaternion norm: {torch.norm(sample['rotation']):.4f} (should be ~1.0)\")\n",
    "    # print(\"Scalar weights mean/std:\", model.spike_to_geo.conv_scalar.weight.mean().item())\n",
    "    # print(\"Vector weights range:\", model.spike_to_geo.conv_vector.weight.min().item())\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "    # Training parameters (from search result [3])\n",
    "    num_epochs = 200\n",
    "    best_val_loss = float('inf')\n",
    "    batch = next(iter(test_loader))\n",
    "    gt_trans = batch['translation'].to(device)\n",
    "    gt_rot = batch['rotation_matrix'].to(device)\n",
    "    with torch.no_grad():\n",
    "        gt_adds = criterion.compute_add_s(gt_trans, gt_rot, gt_trans, gt_rot)\n",
    "        print(\"ADD-S (GT vs GT):\", gt_adds.mean().item())\n",
    "    warmup_epochs = 10\n",
    "    iterations_per_epoch = len(train_loader)\n",
    "    \n",
    "    # END OF ADD BLOCK \n",
    "    for epoch in range(num_epochs):\n",
    "        # Train phase\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n",
    "\n",
    "        # Validation phase\n",
    "        val_loss = validate(model, val_loader, criterion, device, epoch)\n",
    "\n",
    "        #scheduler.step()  # Call without arguments for LambdaLR\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss['total'] < best_val_loss:\n",
    "            best_val_loss = val_loss['total']\n",
    "\n",
    "            # Saving\n",
    "            torch.save({\n",
    "                'model_state': model.state_dict(),\n",
    "                'optimizer_state': optimizer.state_dict() #,\n",
    "                #'scheduler_state': scheduler.state_dict() \n",
    "            }, f'best_s_model_epoch_{epoch}.pth')\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | \"\n",
    "          f\"Val Loss: {val_loss['total']:.4f} | \"\n",
    "          f\"Pos: {val_loss['position']:.4f} | \"\n",
    "          f\"Rot: {val_loss['rotation']:.4f} | \"\n",
    "          f\"ADD-S: {val_loss['add_s']:.4f} | \"\n",
    "          f\"Acc@0.1: {val_loss['acc@0.1']:.3f} | \"\n",
    "          f\"Acc@0.2: {val_loss['acc@0.2']:.3f} | \"\n",
    "          f\"Acc@0.5: {val_loss['acc@0.5']:.3f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156c4ed0",
   "metadata": {},
   "source": [
    "TESTING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd78de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    batch_size = 16\n",
    "    num_steps = 32  # Increased temporal resolution\n",
    "    input_size = (128, 128)\n",
    "    num_epochs = 100\n",
    "    lr = 3e-4\n",
    "    beta = 0.97  # Slower membrane decay\n",
    "    grad_clip = 1.0\n",
    "    irreps_in = o3.Irreps(\"256x0e\")\n",
    "    irreps_hidden = o3.Irreps(\"128x0e + 64x1o + 32x2e\")\n",
    "    irreps_out = o3.Irreps(\"1o + 1o\")  # Rotation + Translation\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "class LearnableTemporalPool(nn.Module):\n",
    "    def __init__(self, num_steps):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=1,  # Process each feature independently\n",
    "            out_channels=1,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "        self.weights = nn.Parameter(torch.ones(num_steps))\n",
    "\n",
    "    def forward(self, spikes):\n",
    "        batch_size, num_steps, channels, height, width = spikes.shape\n",
    "\n",
    "        # Flatten spatial and channel dimensions\n",
    "        x = spikes.reshape(batch_size * channels * height * width, num_steps)\n",
    "\n",
    "        # Add channel dimension: [B*C*H*W, T] -> [B*C*H*W, 1, T]\n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        # Apply convolution: [B*C*H*W, 1, T] -> [B*C*H*W, 1, T]\n",
    "        x = self.conv(x)\n",
    "\n",
    "        # Apply learnable weights: [B*C*H*W, 1, T] * [1, 1, T] -> [B*C*H*W, 1, T]\n",
    "        x = x * self.weights.view(1, 1, -1)\n",
    "\n",
    "        # Sum over time: [B*C*H*W, 1]\n",
    "        x = x.sum(dim=2)\n",
    "\n",
    "        # Reshape to original spatial dimensions: [B, C, H, W]\n",
    "        return x.reshape(batch_size, channels, height, width)\n",
    "\n",
    "class SNNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=3, hidden_dim=64, output_dim=32, beta=0.5, num_steps=25, spike_grad=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(input_dim, hidden_dim, kernel_size=5, stride=2, padding=2, padding_mode='replicate')\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim*2, kernel_size=5, stride=2, padding=2, padding_mode='replicate')\n",
    "        self.conv3 = nn.Conv2d(hidden_dim*2, output_dim, kernel_size=5, stride=2, padding=2, padding_mode='replicate')\n",
    "\n",
    "        self.temporal_pool = LearnableTemporalPool(num_steps)\n",
    "\n",
    "        # Spiking neuron layers\n",
    "        spike_grad = surrogate.fast_sigmoid() if spike_grad is None else spike_grad\n",
    "        self.lif1 = snn.Leaky(\n",
    "            beta=0.85,\n",
    "            threshold=0.5,  # Lower initial threshold\n",
    "            learn_threshold=True,  # Make threshold trainable\n",
    "            spike_grad=surrogate.fast_sigmoid(slope=10.0)\n",
    "        )\n",
    "        self.lif2 = snn.Leaky(\n",
    "            beta=0.78,\n",
    "            threshold=0.6,  # Lower initial threshold\n",
    "            learn_threshold=True,  # Make threshold trainable\n",
    "            spike_grad=surrogate.fast_sigmoid(slope=10.0)\n",
    "        )\n",
    "        self.lif3 = snn.Leaky(\n",
    "            beta=0.72,\n",
    "            threshold=0.7,  # Lower initial threshold\n",
    "            learn_threshold=True,  # Make threshold trainable\n",
    "            spike_grad=surrogate.fast_sigmoid(slope=10.0)\n",
    "        )\n",
    "        # Hidden states\n",
    "        self.mem1 = self.mem2 = self.mem3 = None\n",
    "\n",
    "    def forward(self, x, num_steps=25):\n",
    "        # Reset membrane potentials\n",
    "        batch_size = x.size(0)\n",
    "        x = x.unsqueeze(1)  # [B, 1, C, H, W]\n",
    "        x = x.repeat(1, num_steps, 1, 1, 1)  # [B, T, C, H, W]\n",
    "        x += torch.randn_like(x) * 0.1  # Add temporal noise\n",
    "        x = x.view(batch_size * num_steps, *x.shape[2:])  # [B*T, C, H, W]\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN in SNN input after reshaping!\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        # Initialize membrane potentials\n",
    "        mem1 = torch.zeros(\n",
    "            batch_size,\n",
    "            num_steps,\n",
    "            self.conv1.out_channels,\n",
    "            x.size(2)//2,\n",
    "            x.size(3)//2\n",
    "        ).to(x.device)\n",
    "        mem2 = torch.zeros(\n",
    "            batch_size,\n",
    "            num_steps,\n",
    "            self.conv2.out_channels,\n",
    "            x.size(2)//4,\n",
    "            x.size(3)//4\n",
    "        ).to(x.device)\n",
    "        mem3 = torch.zeros(\n",
    "            batch_size,\n",
    "            num_steps,\n",
    "            self.conv3.out_channels,\n",
    "            x.size(2)//8,\n",
    "            x.size(3)//8\n",
    "        ).to(x.device)\n",
    "        mem1 = self.lif1.init_leaky()\n",
    "        mem2 = self.lif2.init_leaky()\n",
    "        mem3 = self.lif3.init_leaky()\n",
    "\n",
    "        spk3_rec = []\n",
    "        for t in range(num_steps):\n",
    "            x_t = x.view(batch_size, num_steps, *x.shape[1:])[:, t]\n",
    "\n",
    "            # Layer 1 with snnTorch state handling\n",
    "            cur1 = self.conv1(x_t)\n",
    "            spk1, mem1 = self.lif1(cur1, mem1)  # Automatic state update\n",
    "\n",
    "            # Layer 2\n",
    "            cur2 = self.conv2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, mem2)\n",
    "\n",
    "            # Layer 3\n",
    "            cur3 = self.conv3(spk2)\n",
    "            spk3, mem3 = self.lif3(cur3, mem3)\n",
    "\n",
    "            spk3_rec.append(spk3)\n",
    "\n",
    "        return torch.stack(spk3_rec, dim=1)\n",
    "\n",
    "class SpikeToGeometric(nn.Module):\n",
    "    def __init__(self, input_dim=32, output_scalar=32, output_vector=32):\n",
    "        super().__init__()\n",
    "        self.conv_scalar = nn.Conv2d(input_dim, output_scalar, kernel_size=1)\n",
    "        self.conv_vector = nn.Conv2d(input_dim, output_vector*3, kernel_size=1)\n",
    "        self.output_vector = output_vector\n",
    "\n",
    "        # Proper initialization for Conv layers\n",
    "        nn.init.kaiming_normal_(self.conv_scalar.weight,\n",
    "                              mode='fan_in',\n",
    "                              nonlinearity='leaky_relu')\n",
    "        nn.init.kaiming_normal_(self.conv_vector.weight,\n",
    "                              mode='fan_in',\n",
    "                              nonlinearity='leaky_relu')\n",
    "        nn.init.zeros_(self.conv_scalar.bias)\n",
    "        nn.init.zeros_(self.conv_vector.bias)\n",
    "\n",
    "    def forward(self, spike_features):\n",
    "        \"\"\"Convert spike rate features to geometric features with spatial preservation\"\"\"\n",
    "        # Input shape: [B, 32, 16, 16]\n",
    "        # Process scalar features (per-location invariant features)\n",
    "        batch_size = spike_features.size(0)\n",
    "        if torch.isnan(spike_features).any():\n",
    "            print(\"NaN in spike_features input to SpikeToGeometric\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        scalar_features = self.conv_scalar(spike_features)  # [B,64,16,16]\n",
    "        if torch.isnan(scalar_features).any():\n",
    "            print(\"NaN in scalar_features after conv_scalar\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        # Process vector features (per-location equivariant features)\n",
    "        vector_features = self.conv_vector(spike_features)  # [B,192,16,16]\n",
    "        vector_features = vector_features.view(\n",
    "            batch_size, self.output_vector, 3, 16, 16\n",
    "        )  # [B,64,3,16,16]\n",
    "        if torch.isnan(vector_features).any():\n",
    "            print(\"NaN in vector_features after conv_vector\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        return scalar_features, vector_features\n",
    "\n",
    "class GraphConstructor(nn.Module):\n",
    "    def __init__(self, feature_dim=256, k_neighbors=8):\n",
    "        super().__init__()\n",
    "        self.k = k_neighbors\n",
    "        self.feature_projection = nn.Linear(feature_dim, feature_dim)\n",
    "\n",
    "    def forward(self, features, positions=None):\n",
    "        batch_size, num_nodes, _ = features.size()\n",
    "        device = features.device\n",
    "\n",
    "        # Project features\n",
    "        node_features = self.feature_projection(features)\n",
    "\n",
    "        # Build graphs\n",
    "        graphs = []\n",
    "        for b in range(batch_size):\n",
    "            if positions is not None:\n",
    "                # Use 3D spatial positions for KNN\n",
    "                pos = positions[b]\n",
    "                edge_index = self._knn_graph(pos, self.k)\n",
    "            else:\n",
    "                # Fallback to feature similarity graph\n",
    "                feat = node_features[b]\n",
    "                edge_index = self._feature_graph(feat, self.k)\n",
    "\n",
    "            graph = Data(\n",
    "                x=node_features[b],\n",
    "                edge_index=edge_index,\n",
    "                pos=positions[b] if positions is not None else None\n",
    "            )\n",
    "            graphs.append(graph)\n",
    "\n",
    "        return Batch.from_data_list(graphs)\n",
    "\n",
    "    def _feature_graph(self, features, k):\n",
    "        \"\"\"Construct graph based on feature similarity (critical for non-spatial data)\"\"\"\n",
    "        device = features.device\n",
    "        num_nodes = features.size(0)\n",
    "        effective_k = min(k, num_nodes - 1)\n",
    "\n",
    "        # Compute cosine similarity\n",
    "        features_norm = F.normalize(features, p=2, dim=1)\n",
    "        similarity = torch.mm(features_norm, features_norm.t())\n",
    "\n",
    "        # Get top k+1 similar neighbors\n",
    "        _, indices = torch.topk(similarity, k=effective_k+1, largest=True)\n",
    "        indices = indices[:, 1:]  # Remove self-loops\n",
    "\n",
    "        # Create edge index with proper device handling\n",
    "        rows = torch.arange(num_nodes, device=device)[:, None].expand(-1, effective_k)\n",
    "        edge_index = torch.stack([\n",
    "            rows.reshape(-1),\n",
    "            indices.reshape(-1).to(device)\n",
    "        ], dim=0)\n",
    "\n",
    "        return edge_index\n",
    "\n",
    "    def _knn_graph(self, points, k):\n",
    "        \"\"\"3D spatial KNN graph construction with locality preservation\"\"\"\n",
    "        device = points.device\n",
    "        num_nodes = points.size(0)\n",
    "        effective_k = min(k, num_nodes - 1)\n",
    "\n",
    "        # Preserve spatial locality by using grid ordering\n",
    "        if num_nodes == 256:  # 16x16 grid\n",
    "            # Create grid indices\n",
    "            idx = torch.arange(num_nodes, device=device).view(16, 16)\n",
    "\n",
    "            # Horizontal connections\n",
    "            horizontal_edges = []\n",
    "            for i in range(16):\n",
    "                for j in range(15):\n",
    "                    n1 = idx[i, j]\n",
    "                    n2 = idx[i, j+1]\n",
    "                    horizontal_edges.append([n1, n2])\n",
    "                    horizontal_edges.append([n2, n1])\n",
    "\n",
    "            # Vertical connections\n",
    "            vertical_edges = []\n",
    "            for i in range(15):\n",
    "                for j in range(16):\n",
    "                    n1 = idx[i, j]\n",
    "                    n2 = idx[i+1, j]\n",
    "                    vertical_edges.append([n1, n2])\n",
    "                    vertical_edges.append([n2, n1])\n",
    "\n",
    "            # Diagonal connections\n",
    "            diagonal_edges = []\n",
    "            for i in range(15):\n",
    "                for j in range(15):\n",
    "                    n1 = idx[i, j]\n",
    "                    n2 = idx[i+1, j+1]\n",
    "                    diagonal_edges.append([n1, n2])\n",
    "                    diagonal_edges.append([n2, n1])\n",
    "\n",
    "            # Combine all connections\n",
    "            edge_index = torch.tensor(horizontal_edges + vertical_edges + diagonal_edges,\n",
    "                                     device=device).t().contiguous()\n",
    "            return edge_index\n",
    "\n",
    "        # Fallback to geometric KNN for non-grid structures\n",
    "        dist = torch.cdist(points, points)\n",
    "        _, indices = torch.topk(dist, k=effective_k+1, largest=False)\n",
    "        indices = indices[:, 1:]  # Remove self-loops\n",
    "\n",
    "        rows = torch.arange(num_nodes, device=device)[:, None].expand(-1, effective_k)\n",
    "        return torch.stack([rows.reshape(-1), indices.reshape(-1)], dim=0)\n",
    "\n",
    "class SE3EquivariantGNN(nn.Module):\n",
    "    def __init__(self, hidden_dim=32, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.irreps_in = o3.Irreps(\"64x0e + 64x1o\")\n",
    "        self.irreps_hidden = o3.Irreps(\"64x0e + 64x1o\")\n",
    "        self.irreps_edge = o3.Irreps(\"1x0e\")  # Scalar edge features\n",
    "        self.irreps_out = o3.Irreps(\"1x1o + 2x1o\") \n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        # Initial layer with edge integration\n",
    "        self.layers.append(o3.TensorProduct(\n",
    "            self.irreps_in,\n",
    "            self.irreps_edge,\n",
    "            self.irreps_hidden,\n",
    "            instructions=[(i,0,i,'uvu',True, 1.0) for i in range(len(self.irreps_in))],\n",
    "            shared_weights=True,\n",
    "            internal_weights=True\n",
    "        ))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers-2):\n",
    "            self.layers.append(o3.TensorProduct(\n",
    "                self.irreps_hidden,\n",
    "                self.irreps_edge,\n",
    "                self.irreps_hidden,\n",
    "                instructions=[(i,0,i,'uvu', True, 1.0) for i in range(len(self.irreps_hidden))],\n",
    "                shared_weights=True,\n",
    "                path_normalization='element',\n",
    "                internal_weights=True\n",
    "            ))\n",
    "            self.layers.append(self.Float32NormActivation(\n",
    "                self.irreps_hidden,\n",
    "                scalar_nonlinearity=torch.tanh,\n",
    "                epsilon=1e-5\n",
    "            ))\n",
    "            \n",
    "        # Output layer\n",
    "        self.layers.append(o3.Linear(self.irreps_hidden, self.irreps_out))\n",
    "\n",
    "    class Float32NormActivation(e3nn_nn.NormActivation):\n",
    "        def forward(self, features):\n",
    "            original_dtype = features.dtype\n",
    "            # Convert to float32 for stable computation\n",
    "            features = features.float()\n",
    "            features = super().forward(features)\n",
    "            # Convert back to original precision\n",
    "            return features.to(original_dtype)\n",
    "\n",
    "    def forward(self, x, edge_index, pos):\n",
    "        # Compute edge features (radial basis)\n",
    "        if torch.isnan(x).any():\n",
    "            print(\"NaN in GNN input\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        senders, receivers = edge_index\n",
    "        rel_pos = pos[senders] - pos[receivers]\n",
    "        distances = torch.norm(rel_pos, dim=1, keepdim=True)\n",
    "        if torch.isnan(rel_pos).any() or torch.isnan(distances).any():\n",
    "            print(\"NaN in edge features\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        edge_attr = torch.exp(-distances**2)  # [num_edges, 1]\n",
    "        # Message passing loop\n",
    "        for layer_idx, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, o3.TensorProduct):\n",
    "                # Aggregate messages using edge features\n",
    "                messages = layer(x[senders], edge_attr)\n",
    "                if torch.isnan(messages).any():\n",
    "                    print(f\"NaN in messages at layer {layer_idx}\")\n",
    "                    raise RuntimeError\n",
    "\n",
    "                x = scatter(messages, receivers, dim=0, dim_size=x.size(0))\n",
    "                if torch.isnan(x).any():\n",
    "                    print(f\"NaN after scattering at layer {layer_idx}\")\n",
    "                    raise RuntimeError\n",
    "            else:\n",
    "                x = layer(x)\n",
    "                # print(x)\n",
    "                if torch.isnan(x).any():\n",
    "                    print(x)\n",
    "                    print(f\"NaN after layer {layer_idx} ({type(layer).__name__})\")\n",
    "                    raise RuntimeError\n",
    "\n",
    "        # Split output\n",
    "        translation = x[:, :3]  # First 3 components (translation)\n",
    "        translation = x[:, :3]\n",
    "        rotation_6d = x[:, 3:9]  # Raw 6D representation\n",
    "        return translation, rotation_6d\n",
    "\n",
    "    def inference_orthogonalize(self, rotation_6d):\n",
    "        \"\"\"Apply orthogonalization ONLY during inference\"\"\"\n",
    "        a1, a2 = rotation_6d[:, :3], rotation_6d[:, 3:6]\n",
    "        return self.sixd_to_rotation_matrix(a1, a2)\n",
    "\n",
    "    def sixd_to_rotation_matrix(self, a1, a2):\n",
    "        \"\"\"Stable 6D to rotation conversion\"\"\"\n",
    "        # Add epsilon guards\n",
    "        a1 = a1 / (torch.norm(a1, dim=-1, keepdim=True) + 1e-6)\n",
    "        a2 = a2 - (a1 * a2).sum(dim=-1, keepdim=True) * a1\n",
    "        a2 = a2 / (torch.norm(a2, dim=-1, keepdim=True) + 1e-6)\n",
    "        b3 = torch.cross(a1, a2)\n",
    "        R = torch.stack([a1, a2, b3], dim=-1)\n",
    "\n",
    "        return R\n",
    "\n",
    "class NeuromorphicLieGNN(nn.Module):\n",
    "    def __init__(self, input_channels=3, hidden_dim=64, num_steps=25, beta=0.5, num_gnn_layers=3):\n",
    "        super().__init__()\n",
    "\n",
    "        # SNN encoder\n",
    "        self.snn_encoder = SNNEncoder(\n",
    "            input_dim=input_channels,\n",
    "            hidden_dim=hidden_dim,\n",
    "            output_dim=32,\n",
    "            beta=beta\n",
    "        )\n",
    "\n",
    "        # Spike to geometric conversion\n",
    "        self.spike_to_geo = SpikeToGeometric(\n",
    "            input_dim=32,  # Depends on input size and encoder architecture\n",
    "            output_scalar=64,\n",
    "            output_vector=64\n",
    "        )\n",
    "\n",
    "        # Graph construction\n",
    "        self.graph_constructor = GraphConstructor(feature_dim=256, k_neighbors=8)\n",
    "\n",
    "        # SE(3) equivariant GNN\n",
    "        self.se3_gnn = SE3EquivariantGNN(hidden_dim=32, num_layers=num_gnn_layers)\n",
    "\n",
    "        # Parameters\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "    def forward(self, images, positions=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Batch of input images (B, C, H, W)\n",
    "            positions: Optional initial position estimates (B, N, 3)\n",
    "        \"\"\"\n",
    "        batch_size = images.size(0)\n",
    "\n",
    "        # 1. Process through SNN encoder to get spike trains\n",
    "        spike_features = self.snn_encoder(images, self.num_steps)\n",
    "        # print(\"Encoder spikes:\", torch.mean(spike_features).item())\n",
    "        # print(\"Spike features shape:\", spike_features.shape)  # [B,T,C,H,W]  # Add here\n",
    "        if torch.isnan(spike_features).any():\n",
    "            raise ValueError(\"NaN in spike features!\")\n",
    "\n",
    "        # 2. Convert to rate-based representation\n",
    "        rate_features = self.snn_encoder.temporal_pool(spike_features)\n",
    "        # Ensure consistent dtype\n",
    "        rate_features = rate_features.to(torch.float32)\n",
    "        # print(\"Rate features shape:\", rate_features.shape)  # [B,C,H,W]\n",
    "        if torch.isnan(rate_features).any():\n",
    "            raise ValueError(\"NaN in rate features!\")\n",
    "\n",
    "        # 3. Convert to geometric features\n",
    "        scalar_features, vector_features = self.spike_to_geo(rate_features)\n",
    "        if torch.isnan(scalar_features).any() or torch.isnan(vector_features).any():\n",
    "            print(\"NaN in geometric features\")\n",
    "            raise RuntimeError\n",
    "\n",
    "        # print(\"Scalar features:\", scalar_features.abs().mean().item())\n",
    "        # print(\"Vector features:\", vector_features.abs().mean().item())\n",
    "        # print(\"Vector features pre-reshape:\", vector_features.shape)\n",
    "        vector_reshaped = vector_features.view(\n",
    "            batch_size,\n",
    "            self.spike_to_geo.output_vector * 3,  # 64*3=192\n",
    "            16,\n",
    "            16\n",
    "        )\n",
    "        # print(\"Vector features post-reshape:\", vector_reshaped.shape)  # [B,192,16,16]\n",
    "        # 4. Construct graph\n",
    "        # Combine scalar and vector features for graph construction\n",
    "        combined_features = torch.cat([\n",
    "            scalar_features,  # [B,64,16,16]\n",
    "            vector_reshaped\n",
    "        ], dim=1)  # [B,256,16,16]\n",
    "        # print(\"Combined features shape:\", combined_features.shape)  # [B,256,16,16]\n",
    "\n",
    "        # Reshape to spatial nodes [B,256,16,16] -> [B,256,256]\n",
    "        node_features = combined_features.permute(0, 2, 3, 1)  # [B, 16, 16, 256]\n",
    "\n",
    "        # Get normalized grid positions\n",
    "        grid = self._get_grid_positions().to(images.device)\n",
    "        grid = (grid - grid.min()) / (grid.max() - grid.min()) * 2 - 1  # Normalize to [-1,1]\n",
    "\n",
    "        # Expand positions to match batch size\n",
    "        positions = grid.unsqueeze(0).repeat(batch_size, 1, 1, 1)  # [B, 16, 16, 3]\n",
    "\n",
    "        # Flatten spatial dimensions while preserving locality\n",
    "        node_features = node_features.reshape(batch_size, 256, -1)  # [B, 256, 256]\n",
    "        positions = positions.reshape(batch_size, 256, 3)  # [B, 256, 3]\n",
    "\n",
    "        # Build graph with spatial positions\n",
    "        graph = self.graph_constructor(node_features, positions)\n",
    "\n",
    "        # 5. Process through SE(3) equivariant GNN\n",
    "        position, rotation_6d = self.se3_gnn(graph.x, graph.edge_index, graph.pos)\n",
    "\n",
    "\n",
    "        rotation_matrix = self.se3_gnn.inference_orthogonalize(rotation_6d)\n",
    "\n",
    "        position = scatter(position, graph.batch, dim=0, reduce='mean')\n",
    "        rotation_6d = scatter(rotation_6d, graph.batch, dim=0, reduce='mean')\n",
    "        rotation_matrix = scatter(rotation_matrix, graph.batch, dim=0, reduce='mean')\n",
    "\n",
    "        return {\n",
    "            'position': position,\n",
    "            'rotation_6d': rotation_6d,  # For training loss\n",
    "            'rotation_matrix': rotation_matrix,  # For inference/validation\n",
    "            'pose': self._combine_pose(position, rotation_matrix)\n",
    "        }\n",
    "\n",
    "    def _combine_pose(self, position, rotation):\n",
    "        \"\"\"Combine position and rotation into 4x4 transformation matrix\"\"\"\n",
    "        batch_size = position.size(0)\n",
    "\n",
    "        # Convert quaternion to rotation matrix\n",
    "        R = rotation\n",
    "        # Create transformation matrix\n",
    "        T = torch.eye(4).unsqueeze(0).repeat(batch_size, 1, 1).to(position.device)\n",
    "        T[:, :3, :3] = R\n",
    "        T[:, :3, 3] = position\n",
    "\n",
    "        return T\n",
    "\n",
    "    def _get_grid_positions(self):\n",
    "        \"\"\"Generate 3D pseudo-coordinates for spatial graph construction\"\"\"\n",
    "        y, x = torch.meshgrid(\n",
    "            torch.arange(16),\n",
    "            torch.arange(16),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        z = torch.zeros_like(x)  # Add depth dimension\n",
    "        return torch.stack([x.float(), y.float(), z], dim=-1)  # [16,16,3]\n",
    "\n",
    "\n",
    "class PoseLoss(nn.Module):\n",
    "    def __init__(self, scene_points, alpha=0.9):\n",
    "        super().__init__()\n",
    "        # Scene points setup\n",
    "        self.register_buffer('scene_points', self._preprocess_scene_points(scene_points))\n",
    "\n",
    "        # EMA parameters for adaptive weighting\n",
    "        self.alpha = alpha\n",
    "        self.register_buffer('pos_ema', torch.tensor(float('inf')))\n",
    "        self.register_buffer('rot_ema', torch.tensor(float('inf')))\n",
    "        self.register_buffer('add_ema', torch.tensor(float('inf')))\n",
    "\n",
    "    def _farthest_point_sampling(self, points, n_samples):\n",
    "        \"\"\"Memory-efficient FPS without full distance matrix\"\"\"\n",
    "        device = points.device\n",
    "        n_points = points.shape[0]\n",
    "\n",
    "        # Initialize with random point\n",
    "        start_idx = torch.randint(0, n_points, (1,)).item()\n",
    "        selected_indices = [start_idx]\n",
    "\n",
    "        # Initialize min distances\n",
    "        min_dists = torch.full((n_points,), float('inf'), device=device)\n",
    "\n",
    "        # Compute initial distances\n",
    "        dists = torch.norm(points - points[start_idx], dim=1)\n",
    "        min_dists = torch.min(min_dists, dists)\n",
    "\n",
    "        for _ in range(1, min(n_samples, n_points)):\n",
    "            # Find farthest point\n",
    "            next_idx = torch.argmax(min_dists).item()\n",
    "            selected_indices.append(next_idx)\n",
    "\n",
    "            # Compute distances to new point\n",
    "            new_dists = torch.norm(points - points[next_idx], dim=1)\n",
    "\n",
    "            # Update min distances\n",
    "            min_dists = torch.min(min_dists, new_dists)\n",
    "            min_dists[next_idx] = 0  # Avoid reselecting\n",
    "\n",
    "        return points[selected_indices]\n",
    "\n",
    "    def _preprocess_scene_points(self, points):\n",
    "        \"\"\"Fast random subsampling for debugging\"\"\"\n",
    "        if isinstance(points, np.ndarray):\n",
    "            points = torch.from_numpy(points)\n",
    "        points = points.float()\n",
    "\n",
    "        # Simple random sampling instead of FPS\n",
    "        if len(points) > 500:\n",
    "            indices = torch.randperm(len(points))[:500]\n",
    "            points = points[indices]\n",
    "\n",
    "        return points\n",
    "\n",
    "    def _matrix_to_6d(self, rotation_matrix):\n",
    "        \"\"\"Convert rotation matrix to 6D representation\"\"\"\n",
    "        return rotation_matrix[:, :, :2].reshape(rotation_matrix.size(0), 6)\n",
    "\n",
    "    def compute_add_s(self, pred_trans, pred_rot, gt_trans, gt_rot):\n",
    "        scene_points = self.scene_points.to(pred_trans.device)\n",
    "        batch_size = pred_trans.size(0)\n",
    "\n",
    "        # Expand points to [B, N, 3]\n",
    "        scene_points_batch = scene_points.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "\n",
    "        # Stable transformation (avoid einsum)\n",
    "        pred_points = torch.bmm(pred_rot, scene_points_batch.transpose(1,2)).transpose(1,2) + pred_trans.unsqueeze(1)\n",
    "        gt_points = torch.bmm(gt_rot, scene_points_batch.transpose(1,2)).transpose(1,2) + gt_trans.unsqueeze(1)\n",
    "\n",
    "        # Clamped distances\n",
    "        dists = torch.cdist(pred_points, gt_points)\n",
    "        min_dists = dists.min(dim=-1)[0].clamp(max=1.0)  # Prevent outliers\n",
    "        return min_dists.mean(dim=1).mean()\n",
    "\n",
    "    def forward(self, pred, target, epoch=None):\n",
    "        # Position loss\n",
    "        pos_loss = F.smooth_l1_loss(pred['position'], target['position'])\n",
    "\n",
    "        # Rotation loss - direct 6D comparison\n",
    "        R_pred = self.sixd_to_rotation_matrix(pred['rotation_6d'])\n",
    "\n",
    "        # Geodesic rotation loss\n",
    "        rot_loss = self.geodesic_loss(R_pred, target['rotation_matrix'])\n",
    "\n",
    "        # ADD-S loss with orthogonalized rotation\n",
    "        add_s = self.compute_add_s(\n",
    "            pred['position'], R_pred,\n",
    "            target['position'], target['rotation_matrix']\n",
    "        )\n",
    "\n",
    "        # Update EMAs\n",
    "        self.pos_ema = self.alpha * self.pos_ema + (1 - self.alpha) * pos_loss.detach()\n",
    "        self.rot_ema = self.alpha * self.rot_ema + (1 - self.alpha) * rot_loss.detach()\n",
    "        self.add_ema = self.alpha * self.add_ema + (1 - self.alpha) * add_s.detach()\n",
    "\n",
    "        # Compute stable weights with clipping\n",
    "        w_pos = torch.clamp(1.0 / (self.pos_ema + 1e-3), 0.1, 10.0)\n",
    "        w_rot = torch.clamp(1.0 / (self.rot_ema + 1e-3), 0.1, 10.0)\n",
    "        w_add = torch.clamp(1.0 / (self.add_ema + 1e-3), 0.1, 10.0)\n",
    "\n",
    "        # Normalize\n",
    "        total = w_pos + w_rot + w_add + 1e-7\n",
    "        w_pos /= total\n",
    "        w_rot /= total\n",
    "        w_add /= total\n",
    "\n",
    "        total_loss = w_pos * pos_loss + w_rot * rot_loss + w_add * add_s\n",
    "\n",
    "        return {\n",
    "            'total': total_loss,\n",
    "            'position': pos_loss,\n",
    "            'rotation': rot_loss,\n",
    "            'add_s': add_s,\n",
    "            'weights': {\n",
    "                'pos': w_pos.item(),\n",
    "                'rot': w_rot.item(),\n",
    "                'add': w_add.item()\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def geodesic_loss(self, R_pred, R_target):\n",
    "        \"\"\"Stable geodesic distance on SO(3)\"\"\"\n",
    "        R_diff = torch.bmm(R_pred, R_target.transpose(1, 2))\n",
    "        trace = R_diff[:, 0, 0] + R_diff[:, 1, 1] + R_diff[:, 2, 2]\n",
    "        angle = torch.acos(torch.clamp((trace - 1)/2, -1+1e-6, 1-1e-6))\n",
    "        return angle.mean()\n",
    "\n",
    "    def sixd_to_rotation_matrix(self, rotation_6d):\n",
    "        \"\"\"SVD-based stable orthogonalization\"\"\"\n",
    "        a1, a2 = rotation_6d[:, :3], rotation_6d[:, 3:6]\n",
    "\n",
    "        # Form unnormalized matrix\n",
    "        b3 = torch.cross(a1, a2)\n",
    "        R = torch.stack([a1, a2, b3], dim=-1)\n",
    "\n",
    "        # SVD orthogonalization\n",
    "        U, S, Vh = torch.linalg.svd(R)\n",
    "        return U @ Vh.transpose(-1, -2)\n",
    "\n",
    "class SevenScenesPoseDataset(Dataset):\n",
    "    def __init__(self, roots=['/content/7scenes', '/content/7scenes2', '/content/7scenes3'],\n",
    "                 transform=None, split='train', val_test_split_ratio=0.7):\n",
    "        self.roots = roots\n",
    "        self.split = split\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(Config.input_size),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # NEW\n",
    "            transforms.RandomRotation(5),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],  # RGB means\n",
    "                std=[0.229, 0.224, 0.225]     # RGB stds\n",
    "            )\n",
    "        ])\n",
    "        self.val_test_split_ratio = val_test_split_ratio\n",
    "\n",
    "        # Initialize data storage\n",
    "        self.poses_t = []\n",
    "        self.poses_r = []\n",
    "        self.samples = []\n",
    "\n",
    "        def format_sequence_name(seq_str):\n",
    "            \"\"\"Convert raw sequence strings to standardized seq-XX format\"\"\"\n",
    "            # Remove existing 'seq-' prefix if present\n",
    "            clean_str = seq_str.lower().replace('sequence', '').strip()\n",
    "\n",
    "            # Handle numeric formats\n",
    "            if clean_str.isdigit():\n",
    "                seq_num = int(clean_str)\n",
    "                return f\"seq-{seq_num:02d}\"  # Always 2-digit format\n",
    "\n",
    "            # Handle already formatted seq-01 cases\n",
    "            if len(clean_str) == 2 and clean_str.isdigit():\n",
    "                return f\"seq-{clean_str}\"\n",
    "\n",
    "            # Fallback for non-numeric sequences\n",
    "            return f\"seq-{clean_str.zfill(2)}\"\n",
    "\n",
    "        # Collect sequences from all roots\n",
    "        all_train_sequences = []\n",
    "        all_test_sequences = []\n",
    "\n",
    "        for root in self.roots:\n",
    "            train_split_file = os.path.join(root, \"TrainSplit.txt\")\n",
    "            test_split_file = os.path.join(root, \"TestSplit.txt\")\n",
    "\n",
    "            # Load train sequences\n",
    "            if os.path.exists(train_split_file):\n",
    "                with open(train_split_file) as f:\n",
    "                    for line in f:\n",
    "                            raw_seq = line.strip()\n",
    "                            if raw_seq:\n",
    "                                # Format sequence name before adding\n",
    "                                formatted_seq = format_sequence_name(raw_seq)\n",
    "                                all_train_sequences.append((root, formatted_seq))\n",
    "\n",
    "            # Load test sequences\n",
    "            if os.path.exists(test_split_file):\n",
    "                with open(test_split_file) as f:\n",
    "                    for line in f:\n",
    "                            raw_seq = line.strip()\n",
    "                            if raw_seq:\n",
    "                                # Format sequence name before adding\n",
    "                                formatted_seq = format_sequence_name(raw_seq)\n",
    "                                all_test_sequences.append((root, formatted_seq))\n",
    "\n",
    "        # Split validation and test from test sequences\n",
    "        if split in ['val', 'test']:\n",
    "            # Split test sequences into val/test subsets\n",
    "            split_idx = int(len(all_test_sequences) * self.val_test_split_ratio)\n",
    "            if split == 'val':\n",
    "                selected_sequences = all_test_sequences[:split_idx]\n",
    "            else:\n",
    "                selected_sequences = all_test_sequences[split_idx:]\n",
    "        else:\n",
    "            selected_sequences = all_train_sequences\n",
    "\n",
    "        for root, seq in selected_sequences:\n",
    "            seq_num = seq[-1]\n",
    "            seq_name = 'seq-0'+seq_num\n",
    "            seq = seq_name\n",
    "        print(selected_sequences)\n",
    "        # Process selected sequences\n",
    "        for root, seq in selected_sequences:\n",
    "            seq_path = os.path.join(root, seq)\n",
    "            if not os.path.exists(seq_path):\n",
    "                continue\n",
    "\n",
    "            # Extract numeric sequence ID\n",
    "            seq_id = seq.split('-')[-1]\n",
    "\n",
    "            # Collect valid samples\n",
    "            color_files = sorted([\n",
    "                f for f in os.listdir(seq_path)\n",
    "                if f.endswith('color.png')\n",
    "            ])\n",
    "\n",
    "            for cf in color_files:\n",
    "                base = cf.replace('color.png', '')\n",
    "                pose_file = base + 'pose.txt'\n",
    "                pose_path = os.path.join(seq_path, pose_file)\n",
    "                img_path = os.path.join(seq_path, cf)\n",
    "\n",
    "                if os.path.exists(pose_path):\n",
    "                    # Load and process pose\n",
    "                    pose_mat = np.loadtxt(pose_path)\n",
    "                    translation = pose_mat[:3, 3]\n",
    "                    rotation = pose_mat[:3, :3]\n",
    "\n",
    "                    self.poses_t.append(torch.tensor(translation))\n",
    "                    self.poses_r.append(torch.tensor(rotation))\n",
    "                    self.samples.append(img_path)\n",
    "\n",
    "        # Convert to tensors\n",
    "        if self.poses_t:\n",
    "            self.poses_t = torch.stack(self.poses_t)\n",
    "            self.poses_r = torch.stack(self.poses_r)\n",
    "        else:\n",
    "            self.poses_t = torch.empty(0)\n",
    "            self.poses_r = torch.empty(0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image\n",
    "        img_path = self.samples[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # Process rotation matrix\n",
    "        rotation = self.poses_r[idx].float()\n",
    "        translation = self.poses_t[idx].float()\n",
    "\n",
    "        return {\n",
    "            'image': self.transform(image).float(),\n",
    "            'rotation_matrix': rotation,\n",
    "            'translation': translation.clone().detach(),\n",
    "            'path': img_path  # For debugging\n",
    "        }\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device, epoch, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        model.snn_encoder.lif1.reset_mem()\n",
    "        model.snn_encoder.lif2.reset_mem()\n",
    "        model.snn_encoder.lif3.reset_mem()\n",
    "\n",
    "        # Move data to device\n",
    "        images = batch['image'].to(device, dtype=torch.float32)\n",
    "        target_position = batch['translation'].to(device, dtype=torch.float32)\n",
    "        target_rotation = batch['rotation_matrix'].to(device, dtype=torch.float32)\n",
    "\n",
    "        # Skip any NaN batches\n",
    "        if torch.isnan(images).any() or torch.isnan(target_position).any() or torch.isnan(target_rotation).any():\n",
    "            continue\n",
    "\n",
    "        # 1. Forward + loss under autocast\n",
    "        #with torch.cuda.amp.autocast():  # AMP enabled here\n",
    "        outputs = model(images)\n",
    "        pred = {\n",
    "            'position': outputs['position'],\n",
    "            'rotation_6d': outputs['rotation_6d']\n",
    "        }\n",
    "        loss_dict = criterion(pred, {'position': target_position, 'rotation_matrix': target_rotation}, epoch)\n",
    "        loss = loss_dict['total'] / accumulation_steps\n",
    "\n",
    "        # 2. Backward scaled loss\n",
    "        retain_graph = (batch_idx + 1) % accumulation_steps != 0\n",
    "        scaler.scale(loss).backward(retain_graph=retain_graph)\n",
    "\n",
    "        # 3. Step, unscale, clip, update only at accumulation boundary\n",
    "        if (batch_idx + 1) % accumulation_steps == 0:\n",
    "            # Unscale gradients before clipping\n",
    "            scaler.unscale_(optimizer)  # moves gradients back to FP32 for clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip\n",
    "\n",
    "            # Step optimizer and update scaler\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        total_loss += loss_dict['total'].item()\n",
    "\n",
    "    # Handle leftover gradients if dataset size not divisible by accumulation_steps\n",
    "    if len(dataloader) % accumulation_steps != 0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def compute_add_s_accuracy(pred_pts, gt_pts, thresholds=(0.1, 0.2, 0.5)):\n",
    "    \"\"\"Compute ADD-S accuracy at multiple thresholds\"\"\"\n",
    "    # Compute pairwise distances\n",
    "    dists = torch.cdist(pred_pts, gt_pts)\n",
    "\n",
    "    # Get minimum distance for each point\n",
    "    min_dists = dists.min(dim=-1)[0]\n",
    "\n",
    "    # Calculate accuracy for each threshold\n",
    "    accuracies = {}\n",
    "    for thr in thresholds:\n",
    "        within_threshold = (min_dists < thr).float()\n",
    "        accuracies[f'acc@{thr:.1f}'] = within_threshold.mean()\n",
    "\n",
    "    return accuracies\n",
    "\n",
    "def validate(model, dataloader, criterion, device, epoch):\n",
    "    model.eval()\n",
    "    loss_sums = {'total': 0.0, 'position': 0.0, 'rotation': 0.0, 'add_s': 0.0}\n",
    "    acc_sums = {'acc@0.1': 0.0, 'acc@0.2': 0.0, 'acc@0.5': 0.0}\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            images = batch['image'].to(device)\n",
    "            target_pos = batch['translation'].to(device)\n",
    "            target_rot = batch['rotation_matrix'].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, {'position': target_pos, 'rotation_matrix': target_rot}, epoch)\n",
    "\n",
    "            # Accumulate losses\n",
    "            for k in loss_sums:\n",
    "                loss_sums[k] += loss[k].item()\n",
    "\n",
    "            # Compute ADD-S accuracy\n",
    "            scene_pts = criterion.scene_points.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            pts = scene_pts.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "            # Use orthogonalized rotation for accuracy computation\n",
    "            R_pred = criterion.sixd_to_rotation_matrix(outputs['rotation_6d'])\n",
    "            pred_pts = torch.bmm(R_pred, pts.transpose(1,2)).transpose(1,2) + outputs['position'].unsqueeze(1)\n",
    "            gt_pts = torch.bmm(target_rot, pts.transpose(1,2)).transpose(1,2) + target_pos.unsqueeze(1)\n",
    "\n",
    "            batch_acc = compute_add_s_accuracy(pred_pts, gt_pts)\n",
    "\n",
    "            # Accumulate accuracies\n",
    "            for k in acc_sums:\n",
    "                acc_sums[k] += batch_acc[k].item()\n",
    "\n",
    "    # Average across batches\n",
    "    for k in loss_sums:\n",
    "        loss_sums[k] /= num_batches\n",
    "    for k in acc_sums:\n",
    "        acc_sums[k] /= num_batches\n",
    "\n",
    "    # Combine results\n",
    "    return {**loss_sums, **acc_sums}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d9674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    loss_sums = {'total': 0.0, 'position': 0.0, 'rotation': 0.0, 'add_s': 0.0}\n",
    "    acc_sums = {'acc@0.1': 0.0, 'acc@0.2': 0.0, 'acc@0.5': 0.0}\n",
    "    num_batches = len(test_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            target_pos = batch['translation'].to(device)\n",
    "            target_rot = batch['rotation_matrix'].to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, {'position': target_pos, 'rotation_matrix': target_rot})\n",
    "\n",
    "            # Accumulate losses\n",
    "            for k in loss_sums:\n",
    "                loss_sums[k] += loss[k].item()\n",
    "\n",
    "            # Compute ADD-S accuracy\n",
    "            scene_pts = criterion.scene_points.to(device)\n",
    "            batch_size = images.size(0)\n",
    "            pts = scene_pts.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "\n",
    "            # Use orthogonalized rotation for accuracy computation\n",
    "            R_pred = criterion.sixd_to_rotation_matrix(outputs['rotation_6d'])\n",
    "            pred_pts = torch.bmm(R_pred, pts.transpose(1,2)).transpose(1,2) + outputs['position'].unsqueeze(1)\n",
    "            gt_pts = torch.bmm(target_rot, pts.transpose(1,2)).transpose(1,2) + target_pos.unsqueeze(1)\n",
    "\n",
    "            batch_acc = compute_add_s_accuracy(pred_pts, gt_pts)\n",
    "\n",
    "            # Accumulate accuracies\n",
    "            for k in acc_sums:\n",
    "                acc_sums[k] += batch_acc[k].item()\n",
    "\n",
    "    # Average across batches\n",
    "    for k in loss_sums:\n",
    "        loss_sums[k] /= num_batches\n",
    "    for k in acc_sums:\n",
    "        acc_sums[k] /= num_batches\n",
    "\n",
    "    # Combine results\n",
    "    return {**loss_sums, **acc_sums}\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuromorphicLieGNN().to(device)\n",
    "scene_points = pcd_tensor.point[\"positions\"].numpy()/1000.0\n",
    "criterion = PoseLoss(scene_points=scene_points).to(device)\n",
    "checkpoint = torch.load('best_s_model_epoch_62.pth', map_location=device) # Replace file name with your trained model checkpoint\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "test_dataset = SevenScenesPoseDataset(split='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "test_loss = test(model, test_loader, criterion, device)\n",
    "print(test_loss)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
